{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn tutorial: Baseball Analytics in Python \n",
    "\n",
    "This is a quick introduction to doing data analysis with Python - namely the two important libraries \n",
    " 1. For working with SQL-like data tables use [pandas](https://pandas.pydata.org), and\n",
    " 2. for statistical modeling use [scikit-learn](http://scikit-learn.org/stable/).\n",
    "\n",
    "I am essentially copying the nice introduction to scikit-learn that Brad found on [DataCamp](https://www.datacamp.com).  \n",
    "\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/scikit-learn-tutorial-baseball-1 <br>\n",
    "https://www.datacamp.com/community/tutorials/scikit-learn-tutorial-baseball-2\n",
    "\n",
    "I have adapted the first lesson here to use the `pybaseball` module instead of downloanding the data from Sean Lahman's [website.](http://seanlahman.com)\n",
    "\n",
    "We will be working with a historical data set of MLB team's year over year statistics.  By the end we will have a predictive model that estimates the number of wins a team will have at the end of the season from on-field statistics (hits, ERA, strikeouts, runs, etc...).  Before that we will learn some basics of data prepping and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pybaseball\n",
    "\n",
    "This is a Python library for doing analytics on MLB baseball stats.  The GitHub repository for the library is linked below.\n",
    "\n",
    "https://github.com/jldbc/pybaseball\n",
    "\n",
    "The repository provides installation instructions, but essentially **run the next cell.**  After it is finished installing we need to restart this notebook's `kernel` so that library to be available to use.  To do that open the **Kernel** menu above and hitting *'Restart Kernel...'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pybaseball  #To run a notebook cell hit SHIFT+ENTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more, feel free to look around the documentation on the GitHub repository linked below.\n",
    "\n",
    "https://github.com/jldbc/pybaseball/tree/master/docs\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybaseball.lahman as l  # this is how you pull in additional packages\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will collect two sets of data.  One called `teams` will have a variety of data about season by season performance of MLB teams (games played, wins, etc...), and `teams_franchises` contains more historical data (for example, if the team is still active today). \n",
    "\n",
    "Run the next cell to collect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = l.teams()  # this collects the teams data from Lahman's website\n",
    "\n",
    "teams_franchises = l.teams_franchises() # this collects the team franchise data\n",
    "\n",
    "#teams.to_csv(\"teams.csv\")\n",
    "\n",
    "#teams_franchises.to_csv(\"teams_franchises.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teams = pd.read_csv(\"teams.csv\")\n",
    "\n",
    "#teams_franchises = pd.read_csv(\"teams_franchises.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two tables are stored as what is called a pandas `DataFrame` (think of a SQL table) it has rows and columns of typed data.  \n",
    "\n",
    "* To see some basic info on `teams` run the next cell.\n",
    "* To see the first few rows run the cell following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the columns contain data related to a specific team and year. Some of the more important variables are listed below. A full list of the variables can be found [here.](http://seanlahman.com/files/database/readme2016.txt)\n",
    "\n",
    " * `yearID` - Year\n",
    " * `teamID` - Team\n",
    " * `franchID` - Franchise (links to TeamsFranchise table)\n",
    " * `G` - Games played\n",
    " * `W` - Wins\n",
    " * `LgWin` - League Champion(Y or N)\n",
    " * `WSWin` - World Series Winner (Y or N)\n",
    " * `R` - Runs scored\n",
    " * `AB` - At bats\n",
    " * `H` - Hits by batters\n",
    " * `HR` - Homeruns by batters\n",
    " * `BB` - Walks by batters\n",
    " * `SO` - Strikeouts by batters\n",
    " * `SB` - Stolen bases\n",
    " * `CS` - Caught stealing\n",
    " * `HBP` - Batters hit by pitch\n",
    " * `SF` - Sacrifice flies\n",
    " * `RA` - Opponents runs scored\n",
    " * `ER` - Earned runs allowed\n",
    " * `ERA` - Earned run average\n",
    " * `CG` - Complete games\n",
    " * `SHO` - Shutouts\n",
    " * `SV` - Saves\n",
    " * `IPOuts` - Outs Pitched (innings pitched x 3)\n",
    " * `HA` - Hits allowed\n",
    " * `HRA` - Homeruns allowed\n",
    " * `BBA` - Walks allowed\n",
    " * `SOA` - Strikeouts by pitchers\n",
    " * `E` - Errors\n",
    " * `DP` - Double Plays\n",
    " * `FP` - Fielding percentage\n",
    " * `name` - Teamâ€™s full name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Placeholder to do some EDA on `teams` to show off pandas a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # this is the typical alias for importing pandas\n",
    "\n",
    "###Note:  I typically keep all of my import statements in the first cell of a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a powerful library, and one of the nice things is that you can do a wide variety of SQL-like operations with data.  For example, the next cell performs an `inner join` on `teams` and `teams_franchises` to create a DataFrame of active teams that have played in seasons with over 150 games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams = pd.merge(teams[(teams.G >= 150)], # left_df: a mask to filter teams down to row with over 150 games\n",
    "                 teams_franchises[(teams_franchises.active == 'Y')], # right_df: a mask to filter just the acitve teams\n",
    "                 how=\"inner\", # the type of merge or join\n",
    "                 on=\"franchID\") # the column to match the DataFrames on\n",
    "\n",
    "Teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start preparing the data so we can eventually run some statistical models on it.  This tutorial shows how to use sklearn to perform two algorithms\n",
    "1. **K-means clustering** \n",
    "2. **Linear Regression**\n",
    "\n",
    "First let's drop some of the columns that won't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['lgID','franchID','divID','Rank','Ghome',\n",
    "             'L','DivWin','WCWin','LgWin','WSWin','SF',\n",
    "             'name','park','attendance','BPF','PPF',\n",
    "             'teamIDBR','teamIDlahman45','teamIDretro',\n",
    "             'franchID','franchName','active','NAassoc']  # this is a standard Python data type called a list.  \n",
    "\n",
    "\n",
    "Teams.drop(drop_cols, axis=1, inplace=True) # inplace=True allows us to update the data in one step.\n",
    "\n",
    "#Note:  \n",
    "#The default behavior of pandas is to not change the data, \n",
    "#so if you are making changes to a DataFrame you will need to \n",
    "#use the inplace parameter when available or save the changes in a new variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief lesson on null or NA values\n",
    "\n",
    "The `pd.DataFrame.isnull()` method lets you see how many NA values are in a DataFrame.  To see some documentation on this or any python object simply type a `?` after the object you want to learn about.  For example, run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams.isnull?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams.isnull().sum(axis=0).tolist() # this prints the number of NA values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this a little more useful in the next cell. I am using a Python `dictionary` constructed using a *list comprehension* method (the embedded `for` statement). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{list(Teams.columns)[i] : Teams.isnull().sum(axis=0).tolist()[i] \n",
    " for i in range(len(Teams.columns))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating columns with a lot null values\n",
    "Teams = Teams.drop(['CS','HBP'], axis=1)\n",
    "\n",
    "# Filling null values to the median for the remainer\n",
    "Teams['SO'] = Teams['SO'].fillna(Teams['SO'].median())\n",
    "Teams['DP'] = Teams['DP'].fillna(Teams['DP'].median())\n",
    "\n",
    "# Print out null values of all columns of `df`\n",
    "print(Teams.isnull().sum(axis=0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting with Matplotlib\n",
    "\n",
    "There are numerous plotting libraries at your disposal within the python eco-system.  The canonical library is [Matplotlib](https://matplotlib.org).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to display plots within the notebook\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt  # the standard alias for matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distribution of wins\n",
    "plt.hist(Teams['W'], bins=30) # method for histograms\n",
    "plt.xlabel('Wins')\n",
    "plt.ylabel(\"Seasons\")\n",
    "plt.title('Distribution of Wins')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams.W.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to write a Python function\n",
    "\n",
    "Python operates with colons and white space to dictate the logical sequence of the code, and not *curly braces* `{}` that are common in other languages.  This might be a challenge to remember at first, but you get the hang of it, and it helps keep the code tidyer and easier to follow.\n",
    "\n",
    "Template for a function\n",
    "\n",
    "```python\n",
    "def fn(param): #colon\n",
    "    val = param # white space of 4 spaces \n",
    "    for i in [1, 2, 3]: #colon\n",
    "        val = val - i # white space\n",
    "    return val # note that we reduced white space\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins for the win column\n",
    "def assign_win_bins(W): # note the colons (NEEDED AFTER CONTROL FLOW STATEMENTS)\n",
    "    if W < 50: # <- here\n",
    "        return 1\n",
    "    if W >= 50 and W <= 69: # <- here\n",
    "        return 2\n",
    "    if W >= 70 and W <= 89: # <- here\n",
    "        return 3\n",
    "    if W >= 90 and W <= 109: # <- here\n",
    "        return 4\n",
    "    if W >= 110: # <- here\n",
    "        return 5\n",
    "        \n",
    "# Apply `assign_win_bins` to `Teams['W']`    \n",
    "Teams['win_bins'] = Teams['W'].apply(assign_win_bins)\n",
    "\n",
    "Teams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatter graph of Year vs. Wins\n",
    "plt.scatter(Teams['yearID'], Teams['W'], c=Teams['win_bins'])\n",
    "plt.title('Wins Scatter Plot')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Wins')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams = Teams[Teams.yearID >= 1900]  # this is an example where the data change is not save by default and we save a new variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by with Pandas\n",
    "\n",
    "More SQL-like functionality of Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams.groupby(\"yearID\").W.sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = Teams.groupby(\"yearID\")[\"R\", \"G\", \"HR\"].sum() # group by syntax\n",
    "year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = year.assign(mlb_rpg = year.R/year.G) # my prefered syntax for adding a column\n",
    "\n",
    "year.loc[1904:1910, :] # an alternate way to view the first few rows.  This highlights that yearID is now the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create line plot of Year vs. MLB runs per Game\n",
    "plt.plot(year.index, year.mlb_rpg)\n",
    "plt.title('MLB Yearly Runs per Game')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('MLB Runs per Game')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating \"year_label\" column, which will give your algorithm information about how certain years are related \n",
    "# (Dead ball eras, Live ball/Steroid Eras)\n",
    "\n",
    "def assign_label(year):\n",
    "    if year < 1920:\n",
    "        return 1\n",
    "    elif year >= 1920 and year <= 1941:\n",
    "        return 2\n",
    "    elif year >= 1942 and year <= 1945:\n",
    "        return 3\n",
    "    elif year >= 1946 and year <= 1962:\n",
    "        return 4\n",
    "    elif year >= 1963 and year <= 1976:\n",
    "        return 5\n",
    "    elif year >= 1977 and year <= 1992:\n",
    "        return 6\n",
    "    elif year >= 1993 and year <= 2009:\n",
    "        return 7\n",
    "    elif year >= 2010:\n",
    "        return 8\n",
    "        \n",
    "# Add `year_label` column to `Teams`    \n",
    "Teams = Teams.assign(year_label = Teams['yearID'].apply(assign_label))\n",
    "\n",
    "Teams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(Teams['year_label'], prefix='era')\n",
    "\n",
    "# Concatenate `Teams` and `dummy_df`\n",
    "Teams = pd.concat([Teams, dummy_df], axis=1) # the axis=1 statement indicates we are stacking columns\n",
    "\n",
    "Teams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert years into decade bins and creating dummy variables\n",
    "def assign_decade(year):\n",
    "    if year < 1920:\n",
    "        return 1910\n",
    "    elif year >= 1920 and year <= 1929:\n",
    "        return 1920\n",
    "    elif year >= 1930 and year <= 1939:\n",
    "        return 1930\n",
    "    elif year >= 1940 and year <= 1949:\n",
    "        return 1940\n",
    "    elif year >= 1950 and year <= 1959:\n",
    "        return 1950\n",
    "    elif year >= 1960 and year <= 1969:\n",
    "        return 1960\n",
    "    elif year >= 1970 and year <= 1979:\n",
    "        return 1970\n",
    "    elif year >= 1980 and year <= 1989:\n",
    "        return 1980\n",
    "    elif year >= 1990 and year <= 1999:\n",
    "        return 1990\n",
    "    elif year >= 2000 and year <= 2009:\n",
    "        return 2000\n",
    "    elif year >= 2010:\n",
    "        return 2010\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams['decade_label'] = Teams['yearID'].apply(assign_decade)\n",
    "decade_df = pd.get_dummies(Teams['decade_label'], prefix='decade')\n",
    "\n",
    "Teams = pd.concat([Teams, decade_df], axis=1) # the axis=1 statement indicates we are stacking columns\n",
    "\n",
    "Teams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features for Runs per Game and Runs Allowed per Game\n",
    "Teams = Teams.assign(R_per_game = Teams['R'] / Teams['G'] ,\n",
    "                     RA_per_game = Teams['RA'] / Teams['G'])\n",
    "\n",
    "# Merge the mlb_rpg from years back to Teams\n",
    "Teams = pd.merge(Teams, year.loc[:, \"mlb_rpg\"].to_frame(), # the .to_frame() is a technically detail we can discuss later\n",
    "                 how=\"left\", left_on=\"yearID\", right_index=True) # note how we are indicating the join columns\n",
    "\n",
    "# Drop unnecessary columns\n",
    "Teams.drop(['yearID','year_label','decade_label'], axis=1, inplace=True)\n",
    "\n",
    "Teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting wins\n",
    "\n",
    "We are now finished prepping the data and will now show you how to build a model to predict wins in a season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for runs per game vs. wins and runs allowed per game vs. wins\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "ax1.scatter(Teams['R_per_game'], Teams['W'], c='blue')\n",
    "ax1.set_title('Runs per Game vs. Wins')\n",
    "ax1.set_ylabel('Wins')\n",
    "ax1.set_xlabel('Runs per Game')\n",
    "\n",
    "ax2.scatter(Teams['RA_per_game'], Teams['W'], c='red')\n",
    "ax2.set_title('Runs Allowed per Game vs. Wins')\n",
    "ax2.set_xlabel('Runs Allowed per Game')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams.corr()[\"W\"] # Produces the correlations with the column \"W\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the variables\n",
    "\n",
    "Next we will produce clusters in the data from the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['G','R','AB','H','2B','3B','HR',\n",
    "              'BB','SO','SB','RA','ER','ERA','CG',\n",
    "              'SHO','SV','IPouts','HA','HRA','BBA',\n",
    "              'SOA','E','DP','FP','era_1','era_2',\n",
    "              'era_3','era_4','era_5','era_6','era_7',\n",
    "              'era_8','decade_1910','decade_1920',\n",
    "              'decade_1930','decade_1940','decade_1950',\n",
    "              'decade_1960','decade_1970','decade_1980',\n",
    "              'decade_1990','decade_2000','decade_2010',\n",
    "              'R_per_game','RA_per_game', 'mlb_rpg']\n",
    "\n",
    "data_attributes = Teams[attributes]\n",
    "\n",
    "# Print the first rows of `df`\n",
    "data_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from `sklearn` \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create silhouette score dictionary\n",
    "s_score_dict = {}\n",
    "for i in range(2,11):\n",
    "    km = KMeans(n_clusters=i, random_state=1)\n",
    "    l = km.fit_predict(data_attributes)\n",
    "    s_s = metrics.silhouette_score(data_attributes, l)\n",
    "    s_score_dict[i] = [s_s]\n",
    "\n",
    "# Print out `s_score_dict`\n",
    "plt.plot(s_score_dict.keys(), s_score_dict.values());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K-means model and determine euclidian distances for each data point\n",
    "kmeans_model = KMeans(n_clusters=6, random_state=1)\n",
    "distances = kmeans_model.fit_transform(data_attributes)\n",
    "\n",
    "# Create scatter plot using labels from K-means model as color\n",
    "labels = kmeans_model.labels_\n",
    "\n",
    "plt.scatter(distances[:,2], distances[:,5], c=labels)\n",
    "plt.title('Kmeans Clusters')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels from K-means model to `df` DataFrame and attributes list\n",
    "Teams = Teams.assign(labels = labels)\n",
    "\n",
    "decade_df = pd.get_dummies(Teams['labels'], prefix='cluster')\n",
    "\n",
    "Teams = pd.concat([Teams, decade_df], axis=1)\n",
    "\n",
    "Teams.drop('labels', axis=1, inplace=True)\n",
    "\n",
    "for i in range(6):\n",
    "    attributes.append('cluster_{}'.format(i))\n",
    "\n",
    "# Print the first rows of `Teams`\n",
    "Teams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few words about sklearn\n",
    "\n",
    "Sklearn provides a consistent modeling framework (API?) that can produce very much *drag-n-drop* functionality.\n",
    "\n",
    "This is the basic framework\n",
    "\n",
    "```python\n",
    "# Import your favorite algorith, e.g. linear_model, cluster\n",
    "from sklearn.algorithm_family import FavAlgo\n",
    "\n",
    "# Create an instance of the algorithm\n",
    "model = FavAlgo(param_0, param_1, ...)\n",
    "\n",
    "# Fit the model \n",
    "results = model.fit(X, y)\n",
    "\n",
    "# Get results from the model\n",
    "results.predict(X)\n",
    "results.other_neat_stuff(...) # such as coefficients, errors, scores, fitted values, etc...\n",
    "```\n",
    "\n",
    "In addition to the models, sklearn provides a beginning to end framework for modeling, for example data processing and splitting, feature selection, cross validation, and metrics.  For example, here is how you would split your data into a training versus a test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = Teams[attributes], Teams.W\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression` from `sklearn.linear_model`\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import `mean_absolute_error` from `sklearn.metrics`\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Create Linear Regression model, fit model, and make predictions\n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X_train, y_train)\n",
    "# Determine mean absolute error\n",
    "mae_train = mean_absolute_error(y_train, lr.predict(X_train))\n",
    "mae_test = mean_absolute_error(y_test, lr.predict(X_test))\n",
    "\n",
    "# Print `mae`\n",
    "print(\"Training mean error:\", mae_train)\n",
    "print(\"Testing mean error:\", mae_test)\n",
    "\n",
    "print(\"Training R2:\", lr.score(X_train, y_train))\n",
    "print(\"Testing R2:\", lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{attributes[i]: round(lr.coef_[i], 3) for i in range(len(attributes))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lr.predict(X), y, alpha=.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply regularization to the linear model below if you so wish.  Just another example of the *drag-n-drop* nature of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `RidgeCV` from `sklearn.linear_model`\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Create Ridge Linear Regression model, fit model, and make predictions\n",
    "rrm = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0), normalize=True)\n",
    "rrm.fit(X_train, y_train)\n",
    "predictions_rrm = rrm.predict(X_test)\n",
    "\n",
    "# Determine mean absolute error\n",
    "mae_rrm = mean_absolute_error(y_test, predictions_rrm)\n",
    "print(mae_rrm)\n",
    "print(rrm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{attributes[i]: (round(lr.coef_[i], 3), round(rrm.coef_[i], 3)) for i in range(len(attributes))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "For a general introduction to all of the tools.\n",
    "[Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook)\n",
    "\n",
    "For a deeper introduction to Pandas + others.\n",
    "[Python for Data Analysis](https://github.com/wesm/pydata-book)\n",
    "\n",
    "For a deeper introduction to Sklearn.\n",
    "[Introduction to Machine Learning with Python: A Guide for Data Scientist](https://github.com/amueller/introduction_to_ml_with_python)\n",
    "\n",
    "For Python quickly.\n",
    "[A Whirlwind Tour of Python](https://jakevdp.github.io/WhirlwindTourOfPython/)\n",
    "\n",
    "Talk to me if you'd like to borrow any of these - I have digital copies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
